{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPFAe141vPI0G9OZco9ba/3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shcDE/Vision_Paper_Review_Code/blob/main/%EC%9D%B4%EC%9A%B0%EC%A7%84%EA%B5%90%EC%88%98_%EB%85%BC%EB%AC%B8%EA%B5%AC%ED%98%84_%EC%BD%94%EB%93%9C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbjr2APuK66k",
        "outputId": "102a4908-e1f3-439e-cdf0-d3f138f6796e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 0/200] [Batch 0/16] [D loss: 0.6960928440093994] [G loss: 0.6949040293693542]\n",
            "[Epoch 1/200] [Batch 0/16] [D loss: 0.4224514961242676] [G loss: 0.8487991094589233]\n",
            "[Epoch 2/200] [Batch 0/16] [D loss: 0.4301900863647461] [G loss: 0.6969741582870483]\n",
            "[Epoch 3/200] [Batch 0/16] [D loss: 0.5886889100074768] [G loss: 0.5880992412567139]\n",
            "[Epoch 4/200] [Batch 0/16] [D loss: 0.5525980591773987] [G loss: 1.649226188659668]\n",
            "[Epoch 5/200] [Batch 0/16] [D loss: 0.5157387852668762] [G loss: 1.9520888328552246]\n",
            "[Epoch 6/200] [Batch 0/16] [D loss: 0.43164700269699097] [G loss: 1.4265207052230835]\n",
            "[Epoch 7/200] [Batch 0/16] [D loss: 0.3870055675506592] [G loss: 1.2351629734039307]\n",
            "[Epoch 8/200] [Batch 0/16] [D loss: 0.4173913300037384] [G loss: 0.883663535118103]\n",
            "[Epoch 9/200] [Batch 0/16] [D loss: 0.4602462649345398] [G loss: 0.6586522459983826]\n",
            "[Epoch 10/200] [Batch 0/16] [D loss: 0.44308745861053467] [G loss: 0.7645212411880493]\n",
            "[Epoch 11/200] [Batch 0/16] [D loss: 0.5217755436897278] [G loss: 1.195400357246399]\n",
            "[Epoch 12/200] [Batch 0/16] [D loss: 0.4423624277114868] [G loss: 1.5367871522903442]\n",
            "[Epoch 13/200] [Batch 0/16] [D loss: 0.23901675641536713] [G loss: 2.052164316177368]\n",
            "[Epoch 14/200] [Batch 0/16] [D loss: 0.22186234593391418] [G loss: 1.7425159215927124]\n",
            "[Epoch 15/200] [Batch 0/16] [D loss: 0.2886575162410736] [G loss: 1.167257308959961]\n",
            "[Epoch 16/200] [Batch 0/16] [D loss: 0.36538416147232056] [G loss: 0.7841671705245972]\n",
            "[Epoch 17/200] [Batch 0/16] [D loss: 0.33967509865760803] [G loss: 0.8744536638259888]\n",
            "[Epoch 18/200] [Batch 0/16] [D loss: 0.2825605273246765] [G loss: 1.0721168518066406]\n",
            "[Epoch 19/200] [Batch 0/16] [D loss: 0.2796004116535187] [G loss: 1.2391471862792969]\n",
            "[Epoch 20/200] [Batch 0/16] [D loss: 0.24535112082958221] [G loss: 1.926659345626831]\n",
            "[Epoch 21/200] [Batch 0/16] [D loss: 0.1248357892036438] [G loss: 2.4991626739501953]\n",
            "[Epoch 22/200] [Batch 0/16] [D loss: 0.11127122491598129] [G loss: 2.1852312088012695]\n",
            "[Epoch 23/200] [Batch 0/16] [D loss: 0.17050518095493317] [G loss: 1.5853697061538696]\n",
            "[Epoch 24/200] [Batch 0/16] [D loss: 0.1565195769071579] [G loss: 1.5850410461425781]\n",
            "[Epoch 25/200] [Batch 0/16] [D loss: 0.1159246489405632] [G loss: 1.7720685005187988]\n",
            "[Epoch 26/200] [Batch 0/16] [D loss: 0.19830571115016937] [G loss: 1.7812964916229248]\n",
            "[Epoch 27/200] [Batch 0/16] [D loss: 0.08516458421945572] [G loss: 2.7542619705200195]\n",
            "[Epoch 28/200] [Batch 0/16] [D loss: 0.055744051933288574] [G loss: 2.7975361347198486]\n",
            "[Epoch 29/200] [Batch 0/16] [D loss: 0.062796451151371] [G loss: 2.767512798309326]\n",
            "[Epoch 30/200] [Batch 0/16] [D loss: 0.11038193106651306] [G loss: 2.0270776748657227]\n",
            "[Epoch 31/200] [Batch 0/16] [D loss: 0.0753532350063324] [G loss: 2.5716471672058105]\n",
            "[Epoch 32/200] [Batch 0/16] [D loss: 0.04245065152645111] [G loss: 3.5857443809509277]\n",
            "[Epoch 33/200] [Batch 0/16] [D loss: 0.061254605650901794] [G loss: 2.757213592529297]\n",
            "[Epoch 34/200] [Batch 0/16] [D loss: 0.0374576598405838] [G loss: 3.205211877822876]\n",
            "[Epoch 35/200] [Batch 0/16] [D loss: 0.03642819821834564] [G loss: 3.9975974559783936]\n",
            "[Epoch 36/200] [Batch 0/16] [D loss: 0.03099057823419571] [G loss: 3.5211021900177]\n",
            "[Epoch 37/200] [Batch 0/16] [D loss: 0.033631011843681335] [G loss: 3.4298322200775146]\n",
            "[Epoch 38/200] [Batch 0/16] [D loss: 0.027657266706228256] [G loss: 3.30233097076416]\n",
            "[Epoch 39/200] [Batch 0/16] [D loss: 0.02912859246134758] [G loss: 3.5296971797943115]\n",
            "[Epoch 40/200] [Batch 0/16] [D loss: 0.03178831562399864] [G loss: 3.4495654106140137]\n",
            "[Epoch 41/200] [Batch 0/16] [D loss: 0.04440756142139435] [G loss: 3.0271379947662354]\n",
            "[Epoch 42/200] [Batch 0/16] [D loss: 0.031346119940280914] [G loss: 3.1728909015655518]\n",
            "[Epoch 43/200] [Batch 0/16] [D loss: 0.030212081968784332] [G loss: 3.423360824584961]\n",
            "[Epoch 44/200] [Batch 0/16] [D loss: 0.01879742741584778] [G loss: 4.573178291320801]\n",
            "[Epoch 45/200] [Batch 0/16] [D loss: 0.02307107299566269] [G loss: 4.229987144470215]\n",
            "[Epoch 46/200] [Batch 0/16] [D loss: 0.03245740383863449] [G loss: 3.3340952396392822]\n",
            "[Epoch 47/200] [Batch 0/16] [D loss: 0.03371507301926613] [G loss: 6.0286030769348145]\n",
            "[Epoch 48/200] [Batch 0/16] [D loss: 0.01630052551627159] [G loss: 4.2725830078125]\n",
            "[Epoch 49/200] [Batch 0/16] [D loss: 0.017582383006811142] [G loss: 4.1414947509765625]\n",
            "[Epoch 50/200] [Batch 0/16] [D loss: 0.021333057433366776] [G loss: 4.060778617858887]\n",
            "[Epoch 51/200] [Batch 0/16] [D loss: 0.030361825600266457] [G loss: 4.6808977127075195]\n",
            "[Epoch 52/200] [Batch 0/16] [D loss: 0.02704651840031147] [G loss: 3.6280646324157715]\n",
            "[Epoch 53/200] [Batch 0/16] [D loss: 0.025950290262699127] [G loss: 3.385326862335205]\n",
            "[Epoch 54/200] [Batch 0/16] [D loss: 0.020206760615110397] [G loss: 3.6981043815612793]\n",
            "[Epoch 55/200] [Batch 0/16] [D loss: 0.029191073030233383] [G loss: 3.3351891040802]\n",
            "[Epoch 56/200] [Batch 0/16] [D loss: 0.0173446424305439] [G loss: 11.364262580871582]\n",
            "[Epoch 57/200] [Batch 0/16] [D loss: 0.024049902334809303] [G loss: 3.289796829223633]\n",
            "[Epoch 58/200] [Batch 0/16] [D loss: 0.024614643305540085] [G loss: 6.6223649978637695]\n",
            "[Epoch 59/200] [Batch 0/16] [D loss: 0.05946127325296402] [G loss: 3.0608339309692383]\n",
            "[Epoch 60/200] [Batch 0/16] [D loss: 0.08607442677021027] [G loss: 2.4792237281799316]\n",
            "[Epoch 61/200] [Batch 0/16] [D loss: 0.03780882805585861] [G loss: 2.915618419647217]\n",
            "[Epoch 62/200] [Batch 0/16] [D loss: 0.034207507967948914] [G loss: 3.2293829917907715]\n",
            "[Epoch 63/200] [Batch 0/16] [D loss: 0.024929003790020943] [G loss: 3.4352455139160156]\n",
            "[Epoch 64/200] [Batch 0/16] [D loss: 0.037744827568531036] [G loss: 3.405275344848633]\n",
            "[Epoch 65/200] [Batch 0/16] [D loss: 0.016466854140162468] [G loss: 3.578000783920288]\n",
            "[Epoch 66/200] [Batch 0/16] [D loss: 0.01142052747309208] [G loss: 4.199995994567871]\n",
            "[Epoch 67/200] [Batch 0/16] [D loss: 0.029917556792497635] [G loss: 3.1057565212249756]\n",
            "[Epoch 68/200] [Batch 0/16] [D loss: 0.1808013767004013] [G loss: 1.8408098220825195]\n",
            "[Epoch 69/200] [Batch 0/16] [D loss: 0.052275821566581726] [G loss: 2.738544464111328]\n",
            "[Epoch 70/200] [Batch 0/16] [D loss: 0.034872185438871384] [G loss: 2.863367795944214]\n",
            "[Epoch 71/200] [Batch 0/16] [D loss: 0.0071709007024765015] [G loss: 6.569427490234375]\n",
            "[Epoch 72/200] [Batch 0/16] [D loss: 0.014455735683441162] [G loss: 3.706907033920288]\n",
            "[Epoch 73/200] [Batch 0/16] [D loss: 0.023578237742185593] [G loss: 3.271122932434082]\n",
            "[Epoch 74/200] [Batch 0/16] [D loss: 0.039021722972393036] [G loss: 3.7385873794555664]\n",
            "[Epoch 75/200] [Batch 0/16] [D loss: 0.008266608230769634] [G loss: 5.1157097816467285]\n",
            "[Epoch 76/200] [Batch 0/16] [D loss: 0.02970641478896141] [G loss: 3.0816311836242676]\n",
            "[Epoch 77/200] [Batch 0/16] [D loss: 0.023198219016194344] [G loss: 3.3646955490112305]\n",
            "[Epoch 78/200] [Batch 0/16] [D loss: 0.01370521355420351] [G loss: 6.172859191894531]\n",
            "[Epoch 79/200] [Batch 0/16] [D loss: 0.023876460269093513] [G loss: 5.377472877502441]\n",
            "[Epoch 80/200] [Batch 0/16] [D loss: 0.02611706033349037] [G loss: 5.6221022605896]\n",
            "[Epoch 81/200] [Batch 0/16] [D loss: 0.00984223186969757] [G loss: 4.636148929595947]\n",
            "[Epoch 82/200] [Batch 0/16] [D loss: 0.0075264619663357735] [G loss: 4.611560344696045]\n",
            "[Epoch 83/200] [Batch 0/16] [D loss: 0.08583129197359085] [G loss: 3.2985849380493164]\n",
            "[Epoch 84/200] [Batch 0/16] [D loss: 0.020274242386221886] [G loss: 8.17551326751709]\n",
            "[Epoch 85/200] [Batch 0/16] [D loss: 0.01571524143218994] [G loss: 4.715885639190674]\n",
            "[Epoch 86/200] [Batch 0/16] [D loss: 0.06678781658411026] [G loss: 3.3515119552612305]\n",
            "[Epoch 87/200] [Batch 0/16] [D loss: 0.026711154729127884] [G loss: 3.8068673610687256]\n",
            "[Epoch 88/200] [Batch 0/16] [D loss: 0.08021776378154755] [G loss: 3.4024858474731445]\n",
            "[Epoch 89/200] [Batch 0/16] [D loss: 0.12500502169132233] [G loss: 2.9656286239624023]\n",
            "[Epoch 90/200] [Batch 0/16] [D loss: 0.011468796990811825] [G loss: 4.095839977264404]\n",
            "[Epoch 91/200] [Batch 0/16] [D loss: 0.02021162211894989] [G loss: 5.030301094055176]\n",
            "[Epoch 92/200] [Batch 0/16] [D loss: 0.0021884615998715162] [G loss: 8.23650074005127]\n",
            "[Epoch 93/200] [Batch 0/16] [D loss: 0.03952677920460701] [G loss: 5.926766872406006]\n",
            "[Epoch 94/200] [Batch 0/16] [D loss: 0.013963770121335983] [G loss: 4.297900676727295]\n",
            "[Epoch 95/200] [Batch 0/16] [D loss: 0.008827326819300652] [G loss: 4.7327880859375]\n",
            "[Epoch 96/200] [Batch 0/16] [D loss: 0.04043968766927719] [G loss: 8.019932746887207]\n",
            "[Epoch 97/200] [Batch 0/16] [D loss: 0.0054389131255447865] [G loss: 5.031790256500244]\n",
            "[Epoch 98/200] [Batch 0/16] [D loss: 0.009091148152947426] [G loss: 6.333601951599121]\n",
            "[Epoch 99/200] [Batch 0/16] [D loss: 0.0017370358109474182] [G loss: 7.353586673736572]\n",
            "[Epoch 100/200] [Batch 0/16] [D loss: 0.007286175154149532] [G loss: 4.834344863891602]\n",
            "[Epoch 101/200] [Batch 0/16] [D loss: 0.018988538533449173] [G loss: 8.59409236907959]\n",
            "[Epoch 102/200] [Batch 0/16] [D loss: 0.006905741058290005] [G loss: 5.60385274887085]\n",
            "[Epoch 103/200] [Batch 0/16] [D loss: 0.014824477955698967] [G loss: 4.240870475769043]\n",
            "[Epoch 104/200] [Batch 0/16] [D loss: 0.13741359114646912] [G loss: 3.704070806503296]\n",
            "[Epoch 105/200] [Batch 0/16] [D loss: 0.016940584406256676] [G loss: 4.331180095672607]\n",
            "[Epoch 106/200] [Batch 0/16] [D loss: 0.07561412453651428] [G loss: 4.346594333648682]\n",
            "[Epoch 107/200] [Batch 0/16] [D loss: 0.014954956248402596] [G loss: 4.637575626373291]\n",
            "[Epoch 108/200] [Batch 0/16] [D loss: 0.0041631003841757774] [G loss: 5.289368152618408]\n",
            "[Epoch 109/200] [Batch 0/16] [D loss: 0.04788529872894287] [G loss: 6.607277870178223]\n",
            "[Epoch 110/200] [Batch 0/16] [D loss: 0.015336833894252777] [G loss: 4.268507480621338]\n",
            "[Epoch 111/200] [Batch 0/16] [D loss: 0.044816602021455765] [G loss: 5.886486053466797]\n",
            "[Epoch 112/200] [Batch 0/16] [D loss: 0.0015708543360233307] [G loss: 7.118783950805664]\n",
            "[Epoch 113/200] [Batch 0/16] [D loss: 0.02196400985121727] [G loss: 4.844263076782227]\n",
            "[Epoch 114/200] [Batch 0/16] [D loss: 0.012671420350670815] [G loss: 4.720676422119141]\n",
            "[Epoch 115/200] [Batch 0/16] [D loss: 0.0761711597442627] [G loss: 8.433151245117188]\n",
            "[Epoch 116/200] [Batch 0/16] [D loss: 0.0034691705368459225] [G loss: 6.159823894500732]\n",
            "[Epoch 117/200] [Batch 0/16] [D loss: 0.002588410396128893] [G loss: 5.64125919342041]\n",
            "[Epoch 118/200] [Batch 0/16] [D loss: 0.026539599522948265] [G loss: 9.228340148925781]\n",
            "[Epoch 119/200] [Batch 0/16] [D loss: 0.00227555725723505] [G loss: 6.0515031814575195]\n",
            "[Epoch 120/200] [Batch 0/16] [D loss: 0.022909607738256454] [G loss: 5.106863975524902]\n",
            "[Epoch 121/200] [Batch 0/16] [D loss: 0.013136865571141243] [G loss: 5.4173665046691895]\n",
            "[Epoch 122/200] [Batch 0/16] [D loss: 0.42567139863967896] [G loss: 3.89471435546875]\n",
            "[Epoch 123/200] [Batch 0/16] [D loss: 0.01195621956139803] [G loss: 10.492918014526367]\n",
            "[Epoch 124/200] [Batch 0/16] [D loss: 0.04636243358254433] [G loss: 8.832521438598633]\n",
            "[Epoch 125/200] [Batch 0/16] [D loss: 0.010047920048236847] [G loss: 5.044351577758789]\n",
            "[Epoch 126/200] [Batch 0/16] [D loss: 0.00758070033043623] [G loss: 7.055969715118408]\n",
            "[Epoch 127/200] [Batch 0/16] [D loss: 0.004573800601065159] [G loss: 6.32867431640625]\n",
            "[Epoch 128/200] [Batch 0/16] [D loss: 0.011436203494668007] [G loss: 6.7882184982299805]\n",
            "[Epoch 129/200] [Batch 0/16] [D loss: 0.30909132957458496] [G loss: 3.575047016143799]\n",
            "[Epoch 130/200] [Batch 0/16] [D loss: 0.036361221224069595] [G loss: 4.724323749542236]\n",
            "[Epoch 131/200] [Batch 0/16] [D loss: 0.024038923904299736] [G loss: 9.160253524780273]\n",
            "[Epoch 132/200] [Batch 0/16] [D loss: 0.04384399950504303] [G loss: 8.33678913116455]\n",
            "[Epoch 133/200] [Batch 0/16] [D loss: 0.1464136242866516] [G loss: 4.008721828460693]\n",
            "[Epoch 134/200] [Batch 0/16] [D loss: 0.08390166610479355] [G loss: 4.60817813873291]\n",
            "[Epoch 135/200] [Batch 0/16] [D loss: 0.02683780901134014] [G loss: 4.670755863189697]\n",
            "[Epoch 136/200] [Batch 0/16] [D loss: 0.23220881819725037] [G loss: 3.87868332862854]\n",
            "[Epoch 137/200] [Batch 0/16] [D loss: 0.007699532434344292] [G loss: 6.018337726593018]\n",
            "[Epoch 138/200] [Batch 0/16] [D loss: 0.07891912758350372] [G loss: 5.173652648925781]\n",
            "[Epoch 139/200] [Batch 0/16] [D loss: 0.17230841517448425] [G loss: 4.942972183227539]\n",
            "[Epoch 140/200] [Batch 0/16] [D loss: 0.1247168481349945] [G loss: 4.239575386047363]\n",
            "[Epoch 141/200] [Batch 0/16] [D loss: 0.01964864693582058] [G loss: 7.353058338165283]\n",
            "[Epoch 142/200] [Batch 0/16] [D loss: 0.0021868713665753603] [G loss: 6.626317024230957]\n",
            "[Epoch 143/200] [Batch 0/16] [D loss: 0.03053363785147667] [G loss: 11.439918518066406]\n",
            "[Epoch 144/200] [Batch 0/16] [D loss: 0.021390821784734726] [G loss: 4.81796932220459]\n",
            "[Epoch 145/200] [Batch 0/16] [D loss: 0.08109383285045624] [G loss: 4.959841251373291]\n",
            "[Epoch 146/200] [Batch 0/16] [D loss: 0.01920490525662899] [G loss: 6.99839448928833]\n",
            "[Epoch 147/200] [Batch 0/16] [D loss: 0.005290978588163853] [G loss: 5.990363121032715]\n",
            "[Epoch 148/200] [Batch 0/16] [D loss: 0.007458056788891554] [G loss: 5.6526079177856445]\n",
            "[Epoch 149/200] [Batch 0/16] [D loss: 0.008157401345670223] [G loss: 6.210172653198242]\n",
            "[Epoch 150/200] [Batch 0/16] [D loss: 0.0776735246181488] [G loss: 4.372650146484375]\n",
            "[Epoch 151/200] [Batch 0/16] [D loss: 0.0036707413382828236] [G loss: 7.445592880249023]\n",
            "[Epoch 152/200] [Batch 0/16] [D loss: 0.004684182349592447] [G loss: 10.408761024475098]\n",
            "[Epoch 153/200] [Batch 0/16] [D loss: 0.061019398272037506] [G loss: 6.208177089691162]\n",
            "[Epoch 154/200] [Batch 0/16] [D loss: 0.026541508734226227] [G loss: 4.781350612640381]\n",
            "[Epoch 155/200] [Batch 0/16] [D loss: 0.015751147642731667] [G loss: 6.8503594398498535]\n",
            "[Epoch 156/200] [Batch 0/16] [D loss: 0.5173603296279907] [G loss: 4.469455718994141]\n",
            "[Epoch 157/200] [Batch 0/16] [D loss: 0.08483961224555969] [G loss: 6.0119452476501465]\n",
            "[Epoch 158/200] [Batch 0/16] [D loss: 0.04908830672502518] [G loss: 3.660496950149536]\n",
            "[Epoch 159/200] [Batch 0/16] [D loss: 0.1407460868358612] [G loss: 5.295661449432373]\n",
            "[Epoch 160/200] [Batch 0/16] [D loss: 0.011023789644241333] [G loss: 10.042529106140137]\n",
            "[Epoch 161/200] [Batch 0/16] [D loss: 0.11189030855894089] [G loss: 4.317485809326172]\n",
            "[Epoch 162/200] [Batch 0/16] [D loss: 0.05089268833398819] [G loss: 2.977487087249756]\n",
            "[Epoch 163/200] [Batch 0/16] [D loss: 0.035499658435583115] [G loss: 4.840626239776611]\n",
            "[Epoch 164/200] [Batch 0/16] [D loss: 0.011747771874070168] [G loss: 8.977273941040039]\n",
            "[Epoch 165/200] [Batch 0/16] [D loss: 0.007660781033337116] [G loss: 9.541597366333008]\n",
            "[Epoch 166/200] [Batch 0/16] [D loss: 0.5936824083328247] [G loss: 4.017096519470215]\n",
            "[Epoch 167/200] [Batch 0/16] [D loss: 0.15017634630203247] [G loss: 3.8009915351867676]\n",
            "[Epoch 168/200] [Batch 0/16] [D loss: 0.030196210369467735] [G loss: 5.665562629699707]\n",
            "[Epoch 169/200] [Batch 0/16] [D loss: 0.008978506550192833] [G loss: 7.116103649139404]\n",
            "[Epoch 170/200] [Batch 0/16] [D loss: 0.022682495415210724] [G loss: 6.874231815338135]\n",
            "[Epoch 171/200] [Batch 0/16] [D loss: 0.06362787634134293] [G loss: 11.832143783569336]\n",
            "[Epoch 172/200] [Batch 0/16] [D loss: 0.019185323268175125] [G loss: 7.055248737335205]\n",
            "[Epoch 173/200] [Batch 0/16] [D loss: 0.019902609288692474] [G loss: 5.821423530578613]\n",
            "[Epoch 174/200] [Batch 0/16] [D loss: 0.06788133084774017] [G loss: 7.480823040008545]\n",
            "[Epoch 175/200] [Batch 0/16] [D loss: 0.01620083674788475] [G loss: 7.944736957550049]\n",
            "[Epoch 176/200] [Batch 0/16] [D loss: 0.018424691632390022] [G loss: 7.294658660888672]\n",
            "[Epoch 177/200] [Batch 0/16] [D loss: 0.04771905019879341] [G loss: 4.515208721160889]\n",
            "[Epoch 178/200] [Batch 0/16] [D loss: 0.041633814573287964] [G loss: 7.671884536743164]\n",
            "[Epoch 179/200] [Batch 0/16] [D loss: 0.011148168705403805] [G loss: 5.5637993812561035]\n",
            "[Epoch 180/200] [Batch 0/16] [D loss: 0.05436602234840393] [G loss: 5.558464050292969]\n",
            "[Epoch 181/200] [Batch 0/16] [D loss: 0.4287233054637909] [G loss: 4.37453556060791]\n",
            "[Epoch 182/200] [Batch 0/16] [D loss: 0.1298147588968277] [G loss: 3.5342674255371094]\n",
            "[Epoch 183/200] [Batch 0/16] [D loss: 0.05537797510623932] [G loss: 3.9136624336242676]\n",
            "[Epoch 184/200] [Batch 0/16] [D loss: 0.034683708101511] [G loss: 7.5198974609375]\n",
            "[Epoch 185/200] [Batch 0/16] [D loss: 0.006618543528020382] [G loss: 8.628562927246094]\n",
            "[Epoch 186/200] [Batch 0/16] [D loss: 0.03827064484357834] [G loss: 8.52324104309082]\n",
            "[Epoch 187/200] [Batch 0/16] [D loss: 0.02617856301367283] [G loss: 12.909346580505371]\n",
            "[Epoch 188/200] [Batch 0/16] [D loss: 0.12303236126899719] [G loss: 3.341080665588379]\n",
            "[Epoch 189/200] [Batch 0/16] [D loss: 0.02768227644264698] [G loss: 7.153246879577637]\n",
            "[Epoch 190/200] [Batch 0/16] [D loss: 0.04696838557720184] [G loss: 6.002957344055176]\n",
            "[Epoch 191/200] [Batch 0/16] [D loss: 0.06604138016700745] [G loss: 12.871708869934082]\n",
            "[Epoch 192/200] [Batch 0/16] [D loss: 0.05785081908106804] [G loss: 4.346134185791016]\n",
            "[Epoch 193/200] [Batch 0/16] [D loss: 0.04450315609574318] [G loss: 15.89593505859375]\n",
            "[Epoch 194/200] [Batch 0/16] [D loss: 0.11714335530996323] [G loss: 4.036067962646484]\n",
            "[Epoch 195/200] [Batch 0/16] [D loss: 0.041931405663490295] [G loss: 6.3568010330200195]\n",
            "[Epoch 196/200] [Batch 0/16] [D loss: 0.08897542953491211] [G loss: 6.029910087585449]\n",
            "[Epoch 197/200] [Batch 0/16] [D loss: 0.10885345190763474] [G loss: 19.887502670288086]\n",
            "[Epoch 198/200] [Batch 0/16] [D loss: 0.0056679751724004745] [G loss: 6.9590840339660645]\n",
            "[Epoch 199/200] [Batch 0/16] [D loss: 0.1656891107559204] [G loss: 12.27065372467041]\n",
            "Training would proceed...\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define the generator and discriminator architectures\n",
        "# Placeholder for actual neural network architectures\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(100, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 784),  # Assuming the data is the size of a flattened MNIST image\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.model(z)\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(784, 512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(256, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Initialize the networks\n",
        "generator = Generator()\n",
        "discriminator = Discriminator()\n",
        "\n",
        "# Loss functions\n",
        "adversarial_loss = nn.BCELoss()\n",
        "\n",
        "# Optimizers, using Adam as specified in the algorithm\n",
        "optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "\n",
        "# Training hyperparameters\n",
        "epochs = 200\n",
        "batch_size = 64\n",
        "latent_dim = 100\n",
        "\n",
        "# Placeholder for the data loaders\n",
        "# These would be the actual data loaders for the incomplete and complete data\n",
        "# For now, we'll use random noise as a placeholder for the data\n",
        "incomplete_data_loader = torch.utils.data.DataLoader(torch.randn(1000, 784), batch_size=batch_size)\n",
        "complete_data_loader = torch.utils.data.DataLoader(torch.randn(1000, 784), batch_size=batch_size)\n",
        "\n",
        "# Training loop with size adjustment for the last batch in each epoch\n",
        "for epoch in range(epochs):\n",
        "    for i, (incomplete_data, complete_data) in enumerate(zip(incomplete_data_loader, complete_data_loader)):\n",
        "        # Adjust the size of valid and fake labels to match the batch size\n",
        "        current_batch_size = incomplete_data.size(0)\n",
        "        valid = torch.ones(current_batch_size, 1, requires_grad=False)\n",
        "        fake = torch.zeros(current_batch_size, 1, requires_grad=False)\n",
        "\n",
        "        # Configure input\n",
        "        real_data = complete_data.type(torch.FloatTensor)\n",
        "\n",
        "        # -----------------\n",
        "        #  Train Generator\n",
        "        # -----------------\n",
        "        optimizer_G.zero_grad()\n",
        "\n",
        "        # Sample noise as generator input\n",
        "        z = torch.randn(current_batch_size, latent_dim)\n",
        "\n",
        "        # Generate a batch of data\n",
        "        generated_data = generator(z)\n",
        "\n",
        "        # Loss measures generator's ability to fool the discriminator\n",
        "        g_loss = adversarial_loss(discriminator(generated_data), valid)\n",
        "\n",
        "        g_loss.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        # ---------------------\n",
        "        #  Train Discriminator\n",
        "        # ---------------------\n",
        "        optimizer_D.zero_grad()\n",
        "\n",
        "        # Measure discriminator's ability to classify real from generated samples\n",
        "        real_loss = adversarial_loss(discriminator(real_data), valid)\n",
        "        fake_loss = adversarial_loss(discriminator(generated_data.detach()), fake)\n",
        "        d_loss = (real_loss + fake_loss) / 2\n",
        "\n",
        "        d_loss.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # Output training stats\n",
        "        if i % 50 == 0:\n",
        "            print(f\"[Epoch {epoch}/{epochs}] [Batch {i}/{len(incomplete_data_loader)}] [D loss: {d_loss.item()}] [G loss: {g_loss.item()}]\")\n",
        "\n",
        "# Since we are running in a limited execution environment, we will stop the loop here.\n",
        "print(\"Training would proceed...\")\n"
      ]
    }
  ]
}